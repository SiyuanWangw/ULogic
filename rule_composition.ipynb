{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "openai.api_key = \"\" # Set your API key here\n",
    "\n",
    "def get_GPT4_response(input, temp=1.0, max_tokens=256, logit_dict={}, model=\"gpt-4\"):\n",
    "    while True:\n",
    "        try:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are a helpful factual assistant.\" \n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": input\n",
    "                }\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temp,\n",
    "                logit_bias=logit_dict\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            sleep_time = 5\n",
    "            print(e, f\"Sleep {sleep_time} seconds.\")\n",
    "            time.sleep(sleep_time)\n",
    "    # print(completion.usage)\n",
    "    return completion.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "def get_chat_response(inputs_list, temp=1.0, max_tokens=256, logit_dict={}):\n",
    "    while True:\n",
    "        try:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are a helpful factual assistant.\\n\" + inputs_list[0] \n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": inputs_list[1]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": inputs_list[2]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": inputs_list[3]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": inputs_list[4]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": inputs_list[5]\n",
    "                },\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temp,\n",
    "                logit_bias=logit_dict\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            sleep_time = 5\n",
    "            print(e, f\"Sleep {sleep_time} seconds.\")\n",
    "            time.sleep(sleep_time)\n",
    "    return completion.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence_to_conclusion_premise(each_rule):\n",
    "    each_rule = each_rule.strip()\n",
    "\n",
    "    assert \":- \" in each_rule\n",
    "    conclusion, premises = each_rule.split(\":- \")\n",
    "    premises_list = premises.split(\"),\")\n",
    "    for m in range(len(premises_list)):\n",
    "        premises_list[m] = premises_list[m].strip()\n",
    "        if m < len(premises_list) - 1:\n",
    "            premises_list[m] = premises_list[m] + \")\"\n",
    "        elif premises_list[m][-1] == \";\" or premises_list[m][-1] == \".\":\n",
    "            premises_list[m] = premises_list[m][:-1]\n",
    "    return conclusion.strip(), premises_list\n",
    "\n",
    "# parsing a premise/conclusion into arguments\n",
    "def argument_parsing(premise, output_rela=False):\n",
    "    premise = premise.strip()\n",
    "    args_type_list = []\n",
    "    args_vairable_list = []\n",
    "    if premise.count(\"(\") != 1:\n",
    "        if not output_rela:\n",
    "            return args_type_list, args_vairable_list\n",
    "        else:\n",
    "            return None, args_type_list, args_vairable_list   \n",
    "         \n",
    "    rela_end = premise.index(\"(\")\n",
    "    relation = premise[:rela_end]\n",
    "    args_list = [each.strip() for each in premise[rela_end+1:-1].split(\",\")]\n",
    "    for each in args_list:\n",
    "        if \" \" in each:\n",
    "            each_arg_split= each.split()\n",
    "            each_type = \" \".join(each_arg_split[:-1])\n",
    "            each_variable = each_arg_split[-1]\n",
    "            args_type_list.append(each_type)\n",
    "            args_vairable_list.append(each_variable)\n",
    "        else:\n",
    "            if len(each) <= 2:\n",
    "                args_type_list.append(None)\n",
    "                args_vairable_list.append(each)\n",
    "            elif len(each) > 2:\n",
    "                args_type_list.append(each)\n",
    "                args_vairable_list.append(None)\n",
    "    if output_rela:\n",
    "        return relation, args_type_list, args_vairable_list\n",
    "    else:\n",
    "        return args_type_list, args_vairable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most similar premise\n",
    "\n",
    "import numpy as np\n",
    "def find_lcseque(s1, s2): \n",
    "    #  生成字符串长度加1的0矩阵，m用来保存对应位置匹配的结果\n",
    "    m = [ [ 0 for x in range(len(s2)+1) ] for y in range(len(s1)+1) ] \n",
    "    #  d用来记录转移方向\n",
    "    d = [ [ None for x in range(len(s2)+1) ] for y in range(len(s1)+1) ] \n",
    " \n",
    "    for p1 in range(len(s1)): \n",
    "        for p2 in range(len(s2)): \n",
    "            if s1[p1] == s2[p2]:            # 字符匹配成功，则该位置的值为左上方的值加1\n",
    "                m[p1+1][p2+1] = m[p1][p2]+1\n",
    "                d[p1+1][p2+1] = 'ok'          \n",
    "            elif m[p1+1][p2] > m[p1][p2+1]:  # 左值大于上值，则该位置的值为左值，并标记回溯时的方向\n",
    "                m[p1+1][p2+1] = m[p1+1][p2] \n",
    "                d[p1+1][p2+1] = 'left'          \n",
    "            else:                           # 上值大于左值，则该位置的值为上值，并标记方向up\n",
    "                m[p1+1][p2+1] = m[p1][p2+1]   \n",
    "                d[p1+1][p2+1] = 'up'         \n",
    " \n",
    "    (p1, p2) = (len(s1), len(s2)) \n",
    "    s = [] \n",
    "    while m[p1][p2]:    # 不为None时\n",
    "        c = d[p1][p2]\n",
    "        if c == 'ok':   # 匹配成功，插入该字符，并向左上角找下一个\n",
    "            s.append(s1[p1-1])\n",
    "            p1 -= 1\n",
    "            p2 -= 1 \n",
    "        if c =='left':  # 根据标记，向左找下一个\n",
    "            p2 -= 1\n",
    "        if c == 'up':   # 根据标记，向上找下一个\n",
    "            p1 -= 1\n",
    "    s = [each for each in s if len(each) > 0]\n",
    "    return len(s)\n",
    "\n",
    "\n",
    "def get_most_similar_premise(conc_rela, premise_rela_list):\n",
    "    most_index = -1\n",
    "    max_similarity = -1\n",
    "    simi_list = []\n",
    "    for i in range(len(premise_rela_list)):\n",
    "        cur_similarity = find_lcseque(premise_rela_list[i], conc_rela)\n",
    "        simi_list.append(cur_similarity)\n",
    "        if cur_similarity > max_similarity:\n",
    "            most_index = i\n",
    "            max_similarity = cur_similarity\n",
    "    return most_index, max_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts for each domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "# \"Person\"\n",
    "# without \"Region\" and \"Time Period\" in affordance concepts\n",
    "concepts_affordance = {\n",
    "    \"Animal\": [\"dog\", \"cat\", \"dinosaur\"],\n",
    "    \"Plant\": [\"fruit\", \"vegatable\", \"tree\"],\n",
    "    \"Food\": [\"snack\", \"barbecue\", \"ingredient\", \"beverage\"],\n",
    "    \"Alcohol\": [\"wine\", \"beer\", \"spirits\"],\n",
    "    \"Disease\": [\"allergy\", \"cancer\", \"rhinitis\"],\n",
    "    \"Drug\": [\"antibiotics\", \"narcotics\", \"prescription drug\"],\n",
    "    \"Natural Phenomenon\": [\"weather\", \"natural disaster\", \"energy\"],\n",
    "    \"Condition\": [\"climate\", \"symptom\", \"environment\"], \n",
    "    \"Material\": [\"fuel\", \"steel\", \"plastic\", \"wood\", \"stone\"],\n",
    "    \"Substance\": [\"allergen\", \"gas\", \"water\", \"oxygen\"],\n",
    "    \"Furniture\": [\"table\", \"chair\", \"bed\"],\n",
    "    \"Publication\": [\"album\", \"song\", \"book\", \"discography\", \"magazine\", \"poem\", \"musical work\", \"written work\"],\n",
    "    \"Organization\": [\"company\", \"club\", \"party\", \"union\", \"league\", \"community\", \"studio\"],\n",
    "    \"Facility\": ['healthcare facility', 'hospital', 'clinic', 'nursing home', \"pharmacy\",\n",
    "                'educational facility', 'university', 'school', 'library', \"institution\", 'lab',\n",
    "                'recreation facility', 'park', 'amusement park', 'stadium', 'gym', \"museum\", 'theater', \n",
    "                'production facility', \"factory\", \"farm\", \"assembly plant\", \"power plant\", \"brewery\",\n",
    "                'transport facility', \"station\", \"airport\", \"railway\", \"harbour\", \"port\", \"publisher\",\n",
    "                'business facility', 'mall', 'restaurant', 'bank', 'market', \"shop\", \"store\",\n",
    "                'administrative facility', 'government', \"agency\", \"authority\", \"department\",\n",
    "                'religious facility', 'church', 'mosque', 'temple',\n",
    "                'financial institution', \"venue\", \"landmark\", \"gallery\"],\n",
    "    \"Natural Place\": [\"mountain\", \"river\", \"ocean\", \"desert\", \"island\", \"forest\", \"volcano\", \"habitat\", \"area\", \"mine\"],\n",
    "    \"Event\": [\"conference\", \"workshop\", \"celebration\", \"race\", \"activity\"],\n",
    "    \"Show\": [\"movie\", \"tv show\", \"drama\", \"concert\", \"broadcast\", \"opera\", \"cartoon\", \"comedy\"],\n",
    "    \"Artwork\": [\"photograph\", \"painting\", \"sculpture\", \"architecture\", \"building\"],\n",
    "    \"Job\": [\"doctor\", \"teacher\", \"engineer\", \"actor\", \"lawyer\", \"driver\", \"profession\"],\n",
    "    \"Game\": [\"sport\", \"card game\", \"computer game\"],\n",
    "    \"Vehicle\": [\"car\", \"aircraft\", \"ship\", \"bicycle\", \"rocket\"],\n",
    "    \"Tool\": [\"container\", \"weapon\", \"musical instrument\", \"kitchen tool\", \"equipment\"],\n",
    "    \"Technology\": [\"telecommunication\", \"Internet\", \"browser\", \"algorithm\", \"software\"],\n",
    "    \"Electronic Device\": [\"computer\", \"phone\", \"refrigerator\", \"appliance\", \"device\"],\n",
    "    \"Platform\": [\"operating system\", \"social media platform\", \"streaming media platform\", \"e-commerce platform\"],\n",
    "    \"Financial Product\": [\"insurance\", \"stock\", \"bond\"],\n",
    "    \"Skill\": [\"knowledge\", \"language\", \"recipe\", \"method\", \"capability\", \"experience\", \"technique\", \"course\", \"workexperience\"],\n",
    "    \"Authorization\": [\"credential\", \"license\", \"prescription\", \"identification document\", \"ticket\", \"degree\", \"certification\", \"qualification\", \"medicaldegree\"],\n",
    "    \"Legislation\": [\"policy\", \"rule\", \"regulation\", \"law\"]\n",
    "}\n",
    "print(len(concepts_affordance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_location = {\n",
    "    \"Animal\": [\"dog\", \"cat\", \"dinosaur\"],\n",
    "    \"Plant\": [\"fruit\", \"vegatable\", \"tree\"],\n",
    "    \"Food\": [\"snack\", \"barbecue\", \"ingredient\", \"beverage\"],\n",
    "    \"Alcohol\": [\"wine\", \"beer\", \"spirits\"],\n",
    "    \"Disease\": [\"allergy\", \"cancer\", \"rhinitis\"],\n",
    "    \"Drug\": [\"antibiotics\", \"narcotics\", \"prescription drug\"],\n",
    "    \"Natural Phenomenon\": [\"weather\", \"natural disaster\", \"energy\"],\n",
    "    \"Condition\": [\"climate\", \"symptom\", \"environment\", \"crime\"], \n",
    "    \"Material\": [\"fuel\", \"steel\", \"plastic\", \"wood\", \"stone\"],\n",
    "    \"Substance\": [\"allergen\", \"gas\", \"water\", \"oxygen\"],\n",
    "    \"Furniture\": [\"table\", \"chair\", \"bed\"],\n",
    "    \"Publication\": [\"album\", \"song\", \"book\", \"discography\", \"magazine\", \"poem\", \"musical work\", \"written work\"],\n",
    "    \"Organization\": [\"company\", \"club\", \"party\", \"union\", \"league\", \"community\", \"studio\"],\n",
    "    \"Facility\": ['healthcare facility', 'hospital', 'clinic', 'nursing home', \"pharmacy\",\n",
    "                'educational facility', 'university', 'school', 'library', \"institution\", 'lab',\n",
    "                'recreation facility', 'park', 'amusement park', 'stadium', 'gym', \"museum\", 'theater', \n",
    "                'production facility', \"factory\", \"farm\", \"assembly plant\", \"power plant\", \"brewery\",\n",
    "                'transport facility', \"station\", \"airport\", \"railway\", \"harbour\", \"port\", \"publisher\",\n",
    "                'business facility', 'mall', 'restaurant', 'bank', 'market', \"shop\", \"store\",\n",
    "                'administrative facility', 'government', \"agency\", \"authority\", \"department\",\n",
    "                'religious facility', 'church', 'mosque', 'temple',\n",
    "                'financial institution', \"venue\", \"landmark\", \"gallery\"],\n",
    "    \"Natural Place\": [\"mountain\", \"river\", \"ocean\", \"desert\", \"island\", \"forest\", \"volcano\", \"habitat\", \"area\", \"mine\"],\n",
    "    \"Event\": [\"conference\", \"workshop\", \"celebration\", \"race\", \"activity\"],\n",
    "    \"Show\": [\"movie\", \"tv show\", \"drama\", \"concert\", \"broadcast\", \"opera\", \"cartoon\", \"comedy\"],\n",
    "    \"Artwork\": [\"photograph\", \"painting\", \"sculpture\", \"architecture\", \"building\"],\n",
    "    \"Job\": [\"doctor\", \"teacher\", \"engineer\", \"actor\", \"lawyer\", \"driver\", \"profession\"],\n",
    "    \"Game\": [\"sport\", \"card game\", \"computer game\"],\n",
    "    \"Vehicle\": [\"car\", \"aircraft\", \"ship\", \"bicycle\", \"rocket\"],\n",
    "    \"Tool\": [\"container\", \"weapon\", \"musical instrument\", \"kitchen tool\", \"equipment\"],\n",
    "    \"Technology\": [\"telecommunication\", \"Internet\", \"browser\", \"algorithm\", \"software\"],\n",
    "    \"Electronic Device\": [\"computer\", \"phone\", \"refrigerator\", \"appliance\", \"device\"],\n",
    "    \"Platform\": [\"operating system\", \"social media platform\", \"streaming media platform\", \"e-commerce platform\", \"channel\"],\n",
    "    \"Financial Product\": [\"insurance\", \"stock\", \"bond\"],\n",
    "    \"Skill\": [\"knowledge\", \"language\", \"recipe\", \"method\", \"capability\", \"experience\", \"technique\", \"course\", \"workexperience\"],\n",
    "    \"Authorization\": [\"credential\", \"license\", \"prescription\", \"identification document\", \"ticket\", \"degree\", \"certification\", \"qualification\", \"medicaldegree\", \"authority\"],\n",
    "    \"Legislation\": [\"policy\", \"rule\", \"regulation\", \"law\"],\n",
    "    \"Region\": [\"country\", \"city\", \"town\", \"location\", \"state\", \"province\", \"place\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "# \"Person\"\n",
    "concepts_accessibility = {\n",
    "    \"Animal\": [\"dog\", \"cat\", \"dinosaur\"],\n",
    "    \"Plant\": [\"fruit\", \"vegatable\", \"tree\"],\n",
    "    \"Food\": [\"snack\", \"barbecue\", \"ingredient\", \"beverage\"],\n",
    "    \"Alcohol\": [\"wine\", \"beer\", \"spirits\"],\n",
    "    \"Disease\": [\"allergy\", \"cancer\", \"rhinitis\"],\n",
    "    \"Drug\": [\"antibiotics\", \"narcotics\", \"prescription drug\"],\n",
    "    \"Natural Phenomenon\": [\"weather\", \"natural disaster\", \"energy\"],\n",
    "    \"Condition\": [\"climate\", \"symptom\", \"environment\"], \n",
    "    \"Material\": [\"fuel\", \"steel\", \"plastic\", \"wood\", \"stone\"],\n",
    "    \"Substance\": [\"allergen\", \"gas\", \"water\", \"oxygen\", \"pollen\"],\n",
    "    \"Furniture\": [\"table\", \"chair\", \"bed\"],\n",
    "    \"Publication\": [\"album\", \"song\", \"book\", \"discography\", \"magazine\", \"poem\", \"musical work\", \"written work\"],\n",
    "    \"Organization\": [\"company\", \"club\", \"party\", \"union\", \"league\", \"community\", \"studio\"],\n",
    "    \"Facility\": ['healthcare facility', 'hospital', 'clinic', 'nursing home', \"pharmacy\",\n",
    "                'educational facility', 'university', 'school', 'library', \"institution\", 'lab',\n",
    "                'recreation facility', 'park', 'amusement park', 'stadium', 'gym', \"museum\", 'theater', \n",
    "                'production facility', \"factory\", \"farm\", \"assembly plant\", \"power plant\", \"brewery\",\n",
    "                'transport facility', \"station\", \"airport\", \"railway\", \"harbour\", \"port\", \"publisher\",\n",
    "                'business facility', 'mall', 'restaurant', 'bank', 'market', \"shop\", \"store\",\n",
    "                'administrative facility', 'government', \"agency\", \"authority\", \"department\",\n",
    "                'religious facility', 'church', 'mosque', 'temple',\n",
    "                'financial institution', \"venue\", \"landmark\", \"gallery\"],\n",
    "    \"Natural Place\": [\"mountain\", \"river\", \"ocean\", \"desert\", \"island\", \"forest\", \"volcano\", \"habitat\", \"area\", \"mine\"],\n",
    "    \"Event\": [\"conference\", \"workshop\", \"celebration\", \"race\", \"activity\"],\n",
    "    \"Show\": [\"movie\", \"tv show\", \"drama\", \"concert\", \"broadcast\", \"opera\", \"cartoon\", \"comedy\"],\n",
    "    \"Artwork\": [\"photograph\", \"painting\", \"sculpture\", \"architecture\", \"building\"],\n",
    "    \"Job\": [\"doctor\", \"teacher\", \"engineer\", \"actor\", \"lawyer\", \"driver\", \"profession\"],\n",
    "    \"Game\": [\"sport\", \"card game\", \"computer game\"],\n",
    "    \"Vehicle\": [\"car\", \"aircraft\", \"ship\", \"bicycle\", \"rocket\"],\n",
    "    \"Tool\": [\"container\", \"weapon\", \"musical instrument\", \"kitchen tool\", \"equipment\"],\n",
    "    \"Technology\": [\"telecommunication\", \"Internet\", \"browser\", \"algorithm\", \"software\"],\n",
    "    \"Electronic Device\": [\"computer\", \"phone\", \"refrigerator\", \"appliance\", \"device\"],\n",
    "    \"Platform\": [\"operating system\", \"social media platform\", \"streaming media platform\", \"e-commerce platform\", \"channel\"],\n",
    "    \"Financial Product\": [\"insurance\", \"stock\", \"bond\"],\n",
    "    \"Skill\": [\"knowledge\", \"language\", \"recipe\", \"method\", \"capability\", \"experience\", \"technique\", \"course\", \"workexperience\"],\n",
    "    \"Authorization\": [\"credential\", \"license\", \"prescription\", \"identification document\", \"ticket\", \"degree\", \"certification\", \"qualification\", \"medicaldegree\", \"authority\"],\n",
    "    \"Legislation\": [\"policy\", \"rule\", \"regulation\", \"law\"],\n",
    "    \"Time Period\": ['season', 'times', 'period', \"era\", \"dynasty\", \"time\"],\n",
    "    \"Region\": [\"country\", \"city\", \"town\", \"location\", \"state\", \"province\", \"place\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backward_rules_input_chat(conclusion):\n",
    "    system_prompt = f\"According to commonsense knowledge in realistic scenarios, please generate 3 logical rules in both Prolog and natural langauge to describe the compositional premises of the given conclusion. \\n\" +\\\n",
    "        f\"The rules in Prolog should have the same meaning with the rules in natural language. \\n\" + \\\n",
    "        f\"Each rule should contain multiple premises and each premise should contain two variables in (X, Y, Z, Z1, Z2). \\n\"\n",
    "\n",
    "    example1_input = \"Conclusion: CanAccess(Person X, Show Y) \\n\" + \\\n",
    "        \"Rules: \\n\"\n",
    "    exmaple1_output = \"1. CanAccess(Person X, Show Y):- LocatedIn(Person X, Region Z), BroadcastIn(Show Y, Region Z); \\n\" + \\\n",
    "                    \"If Person X is located in Region Z and Show Y is broadcasted in Region Y, then Person X can access Show Y. \\n\" + \\\n",
    "                    \"2. CanAccess(Person X, Show Y):- ProducedAt(Show Y, Time Period Z1), DiedAt(Person X, Time Period Z2), EarlierThan(Time Period Z1, Time Period Z2); \\n\" + \\\n",
    "                    \"If Show Y was produced at Time Period Z1 and Person X died at a later Time Period Z2, then Person X had the chance to access Show Y. \\n\" + \\\n",
    "                    \"3. CanAccess(Person X, Show Y):- NotAfraid(Person X, Animal Z), ActIn(Animal Z, Show Y); \\n\" + \\\n",
    "                    \"If Person X is not afraid of Animal Z that acts in Show Y, then Person X can access Show Y.\\n\"\n",
    "    \n",
    "    example2_input = \"Conclusion: OriginatedFrom(Food X, Region Y) \\n\" + \\\n",
    "        \"Rules: \\n\"\n",
    "    exmaple2_output = \"1. OriginatedFrom(Food X, Region Y):- ProcessedIn(Food X, Facility Z), LocatedIn(Facility Z, Region Y); \\n\" + \\\n",
    "                    \"If Food X is processed in Facility Z and Facility Z is located in Region Y, then Food X is originated from Region Y. \\n\"  + \\\n",
    "                    \"2. OriginatedFrom(Food X, Region Y):- GrowIn(Food X, Natural Place Z), LocatedIn(Natural Place Z, Region Y); \\n\" + \\\n",
    "                    \"If Food X grows in Natural Place Z and Natural Place Z is located in Region Y, then Food X originate from Region Y. \\n\" + \\\n",
    "                    \"3. OriginatedFrom(Food X, Region Y):- CultivatedBy(Food X, Organization Z), BelongTo(Organization Z, Region Y); \\n\" + \\\n",
    "                    \"If Food X is cultivated by Organization Z, and Organization Z belongs to Region Y, then Food X is originated from Region Y. \\n\" \n",
    "\n",
    "    example3_input = f\"Conclusion: {conclusion} \\n\" + \\\n",
    "        f\"Rules: \\n\"\n",
    "\n",
    "    return [system_prompt, example1_input, exmaple1_output, example2_input, exmaple2_output, example3_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables_symbols = [\" X\", \" Y\", \" Z\", \" Z1\", \" Z2\"]\n",
    "variables_token_ids = []\n",
    "for each in all_variables_symbols:\n",
    "    variables_token_ids += encoding.encode(each)\n",
    "variables_logit_dict = {each: 5.0 for each in set(variables_token_ids)}\n",
    "\n",
    "def get_affordance_logits(concept):\n",
    "    properties_list = [\"Age\", \"Price\", \"Money\", \"Height\", \"Length\", \"Weight\", \"Strength\", \"Size\", \"Density\", \"Volume\", \n",
    "    \"Temperature\", \"Hardness\", \"Speed\", \"BoilingPoint\", \"MeltingPoint\", \"Frequency\", \"Decibel\", \"Space\"]\n",
    "    properties_list = [\" \" + each for each in properties_list]\n",
    "    location_prep = [\"in\", \"from\", \"to\", \"at\"]\n",
    "    \n",
    "    all_objects = [\"Person\", \"Region\"] + list(concept.keys())\n",
    "    all_objects = [\"(\" + each for each in all_objects] + [\" \" + each for each in all_objects]\n",
    "    all_tokens = properties_list + all_objects + location_prep\n",
    "    all_tokens = set(all_tokens)\n",
    "    token_ids = []\n",
    "    for each in all_tokens:\n",
    "        token_ids += encoding.encode(each)\n",
    "    \n",
    "    logit_dict = {each: 2.0 for each in set(token_ids)}\n",
    "    logit_dict.update(variables_logit_dict)\n",
    "    \n",
    "    return logit_dict\n",
    "\n",
    "def get_accessibility_logits(concept):\n",
    "    time_prep = [\"In\", \"During\", \"At\", \"From\", \"To\", \"EarlierThan\", \"LaterThan\"]\n",
    "    time_prep = [\" \" + each for each in time_prep[5:]] + time_prep\n",
    "    all_objects = [\"Person\", \"Time Period\", \"Region\"] + list(concept.keys())\n",
    "\n",
    "    all_objects = [\"(\" + each for each in all_objects] + [\" \" + each for each in all_objects]\n",
    "    all_tokens = time_prep + all_objects\n",
    "    all_tokens = set(all_tokens)\n",
    "    token_ids = []\n",
    "    for each in all_tokens:\n",
    "        token_ids += encoding.encode(each)\n",
    "    \n",
    "    logit_dict = {each: 2.0 for each in set(token_ids)}\n",
    "    logit_dict.update(variables_logit_dict)\n",
    "    \n",
    "    return logit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend the rule by backward chaining\n",
    "from tqdm import tqdm\n",
    "def extend_premise(rule_file, output_rule_file, domain=\"affordance\"):\n",
    "    with open(rule_file, 'r') as r_f:\n",
    "        rules = r_f.readlines()\n",
    "\n",
    "    if domain == \"affordance\":\n",
    "        cur_logit_dict = get_affordance_logits(concepts_affordance)\n",
    "    else:\n",
    "        cur_logit_dict = get_accessibility_logits(concepts_accessibility)\n",
    "    with open(output_rule_file, 'w') as w_f:\n",
    "        all_premises = []\n",
    "        for each_rule in tqdm(rules):\n",
    "            each_rule = each_rule.strip()\n",
    "            _, premises_list = parse_sentence_to_conclusion_premise(each_rule)\n",
    "            \n",
    "            for n in range(len(premises_list)):\n",
    "                each_premise = premises_list[n]\n",
    "                if \"than\" in each_premise.lower() or \"same\" in each_premise.lower() or \"similar\" in each_premise.lower():\n",
    "                    continue\n",
    "                # analyze the premise and replace variables with X,Y\n",
    "                relation, args_type_list, args_vairable_list = argument_parsing(each_premise, output_rela=True)\n",
    "                assert len(args_vairable_list) == 2 and len(args_type_list) == 2\n",
    "                new_premise = f\"{relation}({args_type_list[0]} X, {args_type_list[1]} Y)\" \n",
    "                if new_premise not in all_premises:\n",
    "                    all_premises.append(new_premise)\n",
    "                    inputs = get_backward_rules_input_chat(new_premise)\n",
    "                    response = get_chat_response(inputs, temp=0, max_tokens=400, logit_dict=cur_logit_dict)\n",
    "                    w_f.write(response+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_file = \"ScriptData/Primitive/RuleSet/rule_base/final_rules/interaction_symbolic_pos_rules_allpossibility_strict.txt\"\n",
    "output_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extension/interaction_extension.txt\"\n",
    "extend_premise(rule_file, output_file, domain=\"affordance\")\n",
    "\n",
    "rule_file = \"ScriptData/Primitive/RuleSet/rule_base/final_rules/symbolic_interaction_extension_strict.txt\"\n",
    "output_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extension/interaction_extension_again.txt\"\n",
    "extend_premise(rule_file, output_file, domain=\"affordance\")\n",
    "\n",
    "rule_file = \"ScriptData/Primitive/RuleSet/rule_base/final_rules/symbolic_interaction_extension_again_strict.txt\"\n",
    "output_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extension/interaction_extension_again_2.txt\"\n",
    "extend_premise(rule_file, output_file, domain=\"affordance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the rule with variable types, upper variables, not errors\n",
    "# remove the component in premises same as conclusion\n",
    "from tqdm import tqdm\n",
    "\n",
    "def edit_rule(rule_file_name, edit_symbolic_rule_file_name, edit_verbalized_rule_file_name):\n",
    "    with open(rule_file_name, \"r\") as rule_r_f:\n",
    "        rules = rule_r_f.readlines()\n",
    "    \n",
    "    symbolic_rules, verbalized_rules = [], []\n",
    "    for i in range(len(rules)):\n",
    "        if i % 2 == 0:\n",
    "            if \". \" not in rules[i].strip():\n",
    "                print(\"*\"*10, rules[i])\n",
    "            assert \". \" in rules[i]\n",
    "            if \":- \" not in rules[i]:\n",
    "                print(rules[i])\n",
    "            assert \":- \" in rules[i]\n",
    "            symbolic_rules.append(rules[i].split(\". \")[1])\n",
    "        else:\n",
    "            if \". \" in rules[i].strip():\n",
    "                print(rules[i])\n",
    "            assert \". \" not in rules[i].strip()\n",
    "            if not (\":- \" not in rules[i] and \"if\" in rules[i].lower()):\n",
    "                print(rules[i])\n",
    "            assert \":- \" not in rules[i] and \"if\" in rules[i].lower() \n",
    "            verbalized_rules.append(rules[i].strip())\n",
    "    assert len(symbolic_rules) == len(verbalized_rules)\n",
    "    print(len(symbolic_rules), len(verbalized_rules))\n",
    "    \n",
    "    edit_symbolic_rules = []\n",
    "    edit_verbalized_rules = []\n",
    "    edit_num_1 = 0\n",
    "    edit_num_2 = 0\n",
    "    for n, each_rule in tqdm(enumerate(symbolic_rules)):\n",
    "        each_rule = each_rule.replace(\"PersonX\", \"Person X\").replace(\"PersonY\", \"Person Y\").replace(\"Personz1\", \"Person Z1\").replace(\"Personz2\", \"Person Z2\")\n",
    "        next_rule = False\n",
    "        rule = each_rule.strip()\n",
    "\n",
    "        assert \":- \" in rule\n",
    "        conclusion, premises = rule.split(\":- \")\n",
    "        _, conc_args_types, conc_args_variables = argument_parsing(conclusion, output_rela=True)\n",
    "        if None in conc_args_types:\n",
    "            print(\"hh\", each_rule)\n",
    "        assert None not in conc_args_types\n",
    "        if not (\"X\" in conc_args_variables and \"Y\" in conc_args_variables):\n",
    "            print(\"hh\", each_rule)\n",
    "            continue\n",
    "        assert \"X\" in conc_args_variables and \"Y\" in conc_args_variables\n",
    "\n",
    "        variable_type_dict = {}\n",
    "        variable_type_dict[conc_args_variables[0]] = conc_args_types[0]\n",
    "        variable_type_dict[conc_args_variables[1]] = conc_args_types[1]\n",
    "        \n",
    "        premises_list = premises.split(\"),\")\n",
    "        premises_list = [_.strip() for _ in premises_list]\n",
    "        for i in range(len(premises_list)):\n",
    "            if i < len(premises_list) - 1:\n",
    "                premises_list[i] = premises_list[i] + \")\"\n",
    "            elif premises_list[i][-1] == \";\" or premises_list[i][-1] == \".\":\n",
    "                premises_list[i] = premises_list[i][:-1]\n",
    "        \n",
    "        new_premise_list = [] \n",
    "        for each_premise in premises_list:\n",
    "            cur_rela, cur_args_types, cur_args_variables = argument_parsing(each_premise, output_rela=True)\n",
    "            if len(cur_args_variables) == 0:\n",
    "                next_rule = True\n",
    "                break\n",
    "            cur_args_variables = [each.upper() if each is not None else each for each in cur_args_variables]\n",
    "            new_premise = cur_rela + \"(\"\n",
    "            for j in range(len(cur_args_types)):\n",
    "                if cur_args_types[j] is None:\n",
    "                    if cur_args_variables[j] is not None:\n",
    "                        cur_args_types[j] = variable_type_dict.get(cur_args_variables[j], None)\n",
    "                else:\n",
    "                    if cur_args_variables[j] is not None and cur_args_variables[j] not in variable_type_dict:\n",
    "                        variable_type_dict[cur_args_variables[j]] = cur_args_types[j]\n",
    "                new_premise += (str(cur_args_types[j]) + \" \" + str(cur_args_variables[j])).replace(\"None\", \"\").strip()\n",
    "                if j < len(cur_args_types) - 1:\n",
    "                    new_premise += \", \"\n",
    "            new_premise += \")\"\n",
    "            new_premise_list.append(new_premise)\n",
    "        \n",
    "        if not next_rule:\n",
    "            new_rule = conclusion + \":- \" + \", \".join(new_premise_list) + \";\"\n",
    "\n",
    "            edit_symbolic_rules.append(new_rule)\n",
    "            edit_verbalized_rules.append(verbalized_rules[n].strip())\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    print(edit_num_1, edit_num_2)\n",
    "    print(len(edit_symbolic_rules), len(edit_verbalized_rules))\n",
    "\n",
    "    with open(edit_symbolic_rule_file_name, 'w') as w_f_1:\n",
    "        for each in edit_symbolic_rules:\n",
    "            w_f_1.write(each+\"\\n\")\n",
    "    with open(edit_verbalized_rule_file_name, 'w') as w_f_2:\n",
    "        for each in edit_verbalized_rules:\n",
    "            w_f_2.write(each+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for filtering invalid rules\n",
    "def rule_filtering_function(rule, is_single=False, is_compositional=False):\n",
    "    conclusion, premises = rule.split(\":- \")\n",
    "    conc_args_types, conc_args_variables = argument_parsing(conclusion)\n",
    "\n",
    "    if None in conc_args_types or None in conc_args_variables:\n",
    "        print(\"None type\")\n",
    "        return False\n",
    "\n",
    "    premises_list = premises.split(\"),\")\n",
    "    for i in range(len(premises_list)):\n",
    "        premises_list[i] = premises_list[i].strip()\n",
    "        if i < len(premises_list) - 1:\n",
    "            premises_list[i] = premises_list[i] + \")\"\n",
    "        elif premises_list[i][-1] == \";\" or premises_list[i][-1] == \".\":\n",
    "            premises_list[i] = premises_list[i][:-1]\n",
    "\n",
    "    if not is_single:\n",
    "        if is_compositional:\n",
    "            if len(premises_list) < 2 or len(premises_list) > 8:\n",
    "                print(\"Less than two or more than 8 premises\")\n",
    "                return False\n",
    "        else:\n",
    "            if len(premises_list) < 2 or len(premises_list) > 4:\n",
    "                print(\"Less than two or more than 4 premises\")\n",
    "                return False\n",
    "    else:\n",
    "        if len(premises_list) == 1:\n",
    "            cur_args_types, cur_args_variables = argument_parsing(premises_list[0])\n",
    "            if set(cur_args_variables) == set([\"X\", \"Y\"]) and set(cur_args_types) == set(conc_args_types):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif len(premises_list) > 1:\n",
    "            return False\n",
    "\n",
    "    all_args_type_list, all_args_vairable_list = [], []\n",
    "    for each in premises_list:\n",
    "        cur_args_types, cur_args_variables = argument_parsing(each)\n",
    "        if len(cur_args_variables) == 2:\n",
    "            if cur_args_variables[0] == cur_args_variables[1]:\n",
    "                print(rule, \"Same two variables\")\n",
    "                return False\n",
    "            all_args_type_list += cur_args_types\n",
    "            all_args_vairable_list += cur_args_variables\n",
    "        elif len(cur_args_variables) > 2:\n",
    "            print(\"Premise with too many arguments >= 3\")\n",
    "            return False\n",
    "        elif len(cur_args_variables) == 1:\n",
    "            print(\"Premise with only one argument\")\n",
    "            return False\n",
    "            # pass\n",
    "        elif len(cur_args_variables) == 0:\n",
    "            print(\"Premise with no argument\")\n",
    "            return False\n",
    "    \n",
    "    if len(all_args_vairable_list) < 2:\n",
    "        print(\"No premise with two arguments\")\n",
    "        return False\n",
    "    if None in all_args_vairable_list:\n",
    "        print(\"None type 1\")\n",
    "        return False\n",
    "    if None in all_args_type_list:\n",
    "        print(\"None type 2\")\n",
    "        return False\n",
    "\n",
    "    all_args_vairable_list = [each.upper() for each in all_args_vairable_list]\n",
    "    if \"X\" not in all_args_vairable_list or \"Y\" not in all_args_vairable_list:\n",
    "        print(\"No X or No Y\")\n",
    "        return False\n",
    "    \n",
    "    distinct_variables = set(all_args_vairable_list)\n",
    "\n",
    "    for each in distinct_variables:\n",
    "        if each not in [\"X\", \"Y\"] and all_args_vairable_list.count(each) != 2:\n",
    "            print(rule, \"Not Connected graph from X to Y\")\n",
    "            return False\n",
    "        elif each in [\"X\", \"Y\"] and all_args_vairable_list.count(each) != 1:\n",
    "            print(rule, \"Not Connected graph from X to Y\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def filter_invalid_rule(file_name, filter_file_name, verbalized_file_name, filter_vb_file_name, is_single=False, is_compositional=False):\n",
    "    with open(file_name, \"r\") as r_f:\n",
    "        rules = r_f.readlines()\n",
    "    with open(verbalized_file_name, \"r\") as r_f_2:\n",
    "        verbalized_rules = r_f_2.readlines()\n",
    "    print(len(rules), len(verbalized_rules))\n",
    "\n",
    "    valid_rules = []\n",
    "    valid_verbalized_rules = []\n",
    "    for i, each in tqdm(enumerate(rules)):\n",
    "        rule = each.strip()\n",
    "        if rule_filtering_function(rule, is_single=is_single, is_compositional=is_compositional):\n",
    "            valid_rules.append(rule)\n",
    "            if verbalized_rules[i].strip()[-1] == \".\":\n",
    "                valid_verbalized_rules.append(verbalized_rules[i].strip())\n",
    "            else:\n",
    "                valid_verbalized_rules.append(verbalized_rules[i].strip()+\".\")\n",
    "    print(len(valid_rules), len(valid_verbalized_rules))\n",
    "\n",
    "    with open(filter_file_name, 'w') as w_f:\n",
    "        for each in valid_rules:\n",
    "            w_f.write(each+\"\\n\")\n",
    "    with open(filter_vb_file_name, 'w') as w_f_2:\n",
    "        for each in valid_verbalized_rules:\n",
    "            w_f_2.write(each+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primitive_filter(rule_file, verbalized_rule_file, write_file_name, write_vb_file_name, concepts):\n",
    "    with open(rule_file, \"r\") as r_f:\n",
    "        rules = r_f.readlines()\n",
    "    with open(verbalized_rule_file, \"r\") as v_r_f:\n",
    "        verbalized_rules = v_r_f.readlines()\n",
    "    print(len(rules), len(verbalized_rules))\n",
    "\n",
    "    def find_super_concepts(type):\n",
    "        for each in concepts:\n",
    "            if type.lower() in concepts[each]:\n",
    "                return each\n",
    "        return None\n",
    "\n",
    "    constraint_rules = []\n",
    "    constraint_verbalized_rules = []\n",
    "    properties_list = [\"Age\", \"Price\", \"Money\", \"Height\", \"Length\", \"Weight\", \"Strength\", \"Size\", \"Density\", \"Volume\", \n",
    "        \"Temperature\", \"Hardness\", \"Speed\", \"BoilingPoint\", \"MeltingPoint\", \"Frequency\", \"Decibel\", \"Space\"]\n",
    "    candidate_concepts = list(concepts.keys()) + [\"Person\", \"Region\", \"Time Period\"] + properties_list\n",
    "\n",
    "    conc_filte_keywords = [\"Age\", \"Price\", \"Money\", \"Height\", \"Length\", \"Weight\", \"Strength\", \"Size\", \"Density\", \"Volume\", \n",
    "        \"Temperature\", \"Hardness\", \"Speed\", \"Frequency\", \"Decibel\", \"Space\"]\n",
    "    for n, each_rule in enumerate(rules):\n",
    "        each_rule = each_rule.strip()\n",
    "        conclusion, premises_list = parse_sentence_to_conclusion_premise(each_rule)\n",
    "        conc_args_type_list, _ = argument_parsing(conclusion, output_rela=False)\n",
    "        if conc_args_type_list[0] in conc_filte_keywords or conc_args_type_list[1] in conc_filte_keywords:\n",
    "            continue\n",
    "\n",
    "        replace_types = []\n",
    "        jump_to_next = False\n",
    "        for each_premise in [conclusion] + premises_list:\n",
    "            args_type_list, _ = argument_parsing(each_premise, output_rela=False)\n",
    "            if args_type_list[0] not in candidate_concepts:\n",
    "                super_type = find_super_concepts(args_type_list[0])\n",
    "                if super_type is not None and super_type != args_type_list[1] and super_type not in conc_args_type_list:\n",
    "                    if [args_type_list[0], super_type] not in replace_types:\n",
    "                        replace_types.append([args_type_list[0], super_type])\n",
    "                else:\n",
    "                    jump_to_next = True\n",
    "                    break\n",
    "            elif len(args_type_list) > 1 and args_type_list[1] not in candidate_concepts:\n",
    "                super_type = find_super_concepts(args_type_list[1])\n",
    "                if super_type is not None and super_type != args_type_list[0] and super_type not in conc_args_type_list:\n",
    "                    if [args_type_list[1], super_type] not in replace_types:\n",
    "                        replace_types.append([args_type_list[1], super_type])\n",
    "                else:\n",
    "                    jump_to_next = True\n",
    "                    break\n",
    "        if jump_to_next:\n",
    "            continue\n",
    "        else:\n",
    "            each_verbalized_rule = verbalized_rules[n].strip()\n",
    "            for each_pair in replace_types:\n",
    "                each_rule = each_rule.replace(each_pair[0], each_pair[1])\n",
    "                each_verbalized_rule = each_verbalized_rule.replace(each_pair[0], each_pair[1]).replace(each_pair[0][0].lower() + each_pair[0][1:], each_pair[1])\n",
    "        if each_rule not in constraint_rules:\n",
    "            constraint_rules.append(each_rule)\n",
    "            constraint_verbalized_rules.append(each_verbalized_rule)\n",
    "    print(len(constraint_rules), len(constraint_verbalized_rules))\n",
    "\n",
    "    with open(write_file_name, 'w') as w_f:\n",
    "        for each in constraint_rules:\n",
    "            w_f.write(each+\"\\n\")\n",
    "    with open(write_vb_file_name, 'w') as w_f_2:\n",
    "        for each in constraint_verbalized_rules:\n",
    "            w_f_2.write(each+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affordance_verbalized_critic_input(each_rule):    \n",
    "    input = \"True or False? Please predict whether the input rule is accurate or not according to commonsense knowledge in realistic scenarios, and also explain why. \\n\\nExamples:\\n\" + \\\n",
    "            \"Input: If Person X was born in Season Z and Plant Y blooms in the same Season Z, then Person X can access Plant Y. \\n\" + \\\n",
    "            \"Output: False. Because the season of a person's birth and the blooming season of a plant has no logical connection. \\n\" + \\\n",
    "            \"Input: If Person X has an Age Z1 and Vehicle Y requires an Age above Z2 for driving, with Age Z1 being greater than Age Z2, then Person X can drive Vehicle Y. \\n\" + \\\n",
    "            \"Output: True. Because driving vehicle has a minimum age requirement. \\n\" + \\\n",
    "            \"Input: If Person X has Capital Z1 and the minimum capital requirement for establishing Organization Y is Capital Z2, and Capital Z1 is bigger than Capital Z2, then Person X can establish Organization Y. \\n\" + \\\n",
    "            \"Output: False. Because person can not have a capital and capital is not suitable for value comparison. \\n\" + \\\n",
    "            \"Input: If Person X is allergic to Material Z1, and Clothing Y contains Material Z1, then Person X cannot wear Clothing Y. \\n\" + \\\n",
    "            \"Output: True. Because person should avoid contact with allergenic substances. \\n\\n\" + \\\n",
    "            \"Input: \" + each_rule + \"\\n\" + \\\n",
    "            \"Output:\\n\"\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accessibility_verbalized_critic_input(each_rule):    \n",
    "    input = \"True or False? Please predict whether the input rule is accurate or not according to commonsense knowledge in realistic scenarios, and also explain why. \\n\\nExamples:\\n\" + \\\n",
    "            \"Input: If Person X was born in Season Z and Plant Y blooms in the same Season Z, then Person X can access Plant Y.\\n\" + \\\n",
    "            \"Output: False. Because person is alive in all seasons and can access plant no matter what season it blooms in.  \\n\" + \\\n",
    "            \"Input: If Person X lives in Region Z and Animal Y inhabits the same Region Z, then Person X can access Animal Y.\\n\" + \\\n",
    "            \"Output: True. Because person and animal exist in the same region. \\n\\n\" + \\\n",
    "            \"Input: \" + each_rule + \"\\n\" + \\\n",
    "            \"Output:\\n\"\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_verbalized_critic_input(each_rule):    \n",
    "    input = \"True or False? Please predict whether the input rule is accurate or not according to commonsense knowledge in realistic scenarios, and also explain why. \\n\\nExamples:\\n\" + \\\n",
    "            \"Input: If Person X is born in City Z and City Z is located in Region Y, then Person X lives in Region Y.\\n\" + \\\n",
    "            \"Output: False. Because the place of birth is not always indicative of the current place of residence. \\n\" + \\\n",
    "            \"Input: If Person X attends School Z and School Z is located in Region Y, then Person X studies in Region Y.\\n\" + \\\n",
    "            \"Output: True. Because if a person attends a school, then the region in which they study is the region where the school is located. \\n\\n\" + \\\n",
    "            \"Input: \" + each_rule + \"\\n\" + \\\n",
    "            \"Output:\\n\"\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def rule_critic(rule_file, verbalized_rule_file, write_file_name, write_vb_file_name, rule_type=\"affordance\"):\n",
    "    # classify the verbalized rules via GPT-4\n",
    "    with open(rule_file, \"r\") as r_f:\n",
    "        rules = r_f.readlines()\n",
    "    with open(verbalized_rule_file, \"r\") as v_r_f:\n",
    "        verbalized_rules = v_r_f.readlines()\n",
    "    print(len(rules), len(verbalized_rules))\n",
    "    \n",
    "    verbalized_label_list = []\n",
    "    for each_rule in tqdm(verbalized_rules):\n",
    "        if rule_type==\"affordance\":\n",
    "            v_critic_input = get_affordance_verbalized_critic_input(each_rule.strip())\n",
    "        elif rule_type == \"accessibility\":\n",
    "            v_critic_input = get_accessibility_verbalized_critic_input(each_rule.strip())\n",
    "        elif rule_type == \"location\":\n",
    "            v_critic_input = get_location_verbalized_critic_input(each_rule.strip())\n",
    "        v_response = get_GPT4_response(v_critic_input, max_tokens=40, temp=0)\n",
    "        verbalized_label_list.append(v_response)\n",
    "\n",
    "    print(len(verbalized_label_list))\n",
    "    print(len(rules), len(verbalized_rules))\n",
    "\n",
    "    critic_symbolic_rules = []\n",
    "    critic_verbalized_rules = []\n",
    "    for i in range(len(verbalized_label_list)):\n",
    "        if \"True\" in verbalized_label_list[i]:\n",
    "            if rules[i].strip() not in critic_symbolic_rules:\n",
    "                critic_symbolic_rules.append(rules[i].strip())\n",
    "                critic_verbalized_rules.append(verbalized_rules[i].strip())\n",
    "    print(len(critic_symbolic_rules), len(critic_verbalized_rules), len(verbalized_label_list)-len(critic_symbolic_rules))\n",
    "\n",
    "    with open(write_file_name, 'w') as w_f:\n",
    "        for each in critic_symbolic_rules:\n",
    "            w_f.write(each+\"\\n\")\n",
    "    with open(write_vb_file_name, 'w') as w_f_2:\n",
    "        for each in critic_verbalized_rules:\n",
    "            w_f_2.write(each+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extension/interaction_extension.txt\"\n",
    "# Step 3:\n",
    "# Step 3-1: heuristicaly filter\n",
    "edit_symbolic_rule_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/symbolic_interaction_extension.txt'\n",
    "edit_verbalized_rule_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/verbalized_interaction_extension.txt'\n",
    "edit_rule(output_file, edit_symbolic_rule_file_name, edit_verbalized_rule_file_name)\n",
    "\n",
    "filter_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/filter/symbolic_interaction_extension.txt'\n",
    "filter_verbalized_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/filter/verbalized_interaction_extension.txt'\n",
    "filter_invalid_rule(edit_symbolic_rule_file_name, filter_file_name, edit_verbalized_rule_file_name, filter_verbalized_file_name, is_single=False)\n",
    "\n",
    "# Step 3-2: primitive concept filter\n",
    "constraint_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/constraint/symbolic_interaction_extension.txt'\n",
    "constraint_vb_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/constraint/verbalized_interaction_extension.txt'\n",
    "primitive_filter(filter_file_name, filter_verbalized_file_name, constraint_file_name, constraint_vb_file_name, concepts_accessibility)\n",
    "\n",
    "# Step 3-3: rule critic by GPT-4\n",
    "critic_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/critic/symbolic_interaction_extension.txt'\n",
    "critic_vb_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/critic/verbalized_interaction_extension.txt'\n",
    "rule_critic(constraint_file_name, constraint_vb_file_name, critic_file_name, critic_vb_file_name, rule_type=\"affordance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extension/interaction_extension_again.txt\"\n",
    "# Step 3:\n",
    "# Step 3-1: heuristicaly filter\n",
    "edit_symbolic_rule_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/symbolic_interaction_extension_again.txt'\n",
    "edit_verbalized_rule_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/verbalized_interaction_extension_again.txt'\n",
    "edit_rule(output_file, edit_symbolic_rule_file_name, edit_verbalized_rule_file_name)\n",
    "\n",
    "filter_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/filter/symbolic_interaction_extension_again.txt'\n",
    "filter_verbalized_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/filter/verbalized_interaction_extension_again.txt'\n",
    "filter_invalid_rule(edit_symbolic_rule_file_name, filter_file_name, edit_verbalized_rule_file_name, filter_verbalized_file_name, is_single=False)\n",
    "\n",
    "# Step 3-2: primitive concept filter\n",
    "constraint_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/constraint/symbolic_interaction_extension_again.txt'\n",
    "constraint_vb_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/constraint/verbalized_interaction_extension_again.txt'\n",
    "primitive_filter(filter_file_name, filter_verbalized_file_name, constraint_file_name, constraint_vb_file_name, concepts_accessibility)\n",
    "\n",
    "# Step 3-3: rule critic by GPT-4\n",
    "critic_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/critic/symbolic_interaction_extension_again.txt'\n",
    "critic_vb_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/critic/verbalized_interaction_extension_again.txt'\n",
    "rule_critic(constraint_file_name, constraint_vb_file_name, critic_file_name, critic_vb_file_name, rule_type=\"affordance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extension/interaction_extension_again_2.txt\"\n",
    "# Step 3:\n",
    "# Step 3-1: heuristicaly filter\n",
    "edit_symbolic_rule_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/symbolic_interaction_extension_again_2.txt'\n",
    "edit_verbalized_rule_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/verbalized_interaction_extension_again_2.txt'\n",
    "edit_rule(output_file, edit_symbolic_rule_file_name, edit_verbalized_rule_file_name)\n",
    "\n",
    "filter_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/filter/symbolic_interaction_extension_again_2.txt'\n",
    "filter_verbalized_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/filter/verbalized_interaction_extension_again_2.txt'\n",
    "filter_invalid_rule(edit_symbolic_rule_file_name, filter_file_name, edit_verbalized_rule_file_name, filter_verbalized_file_name, is_single=False)\n",
    "\n",
    "# Step 3-2: primitive concept filter\n",
    "constraint_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/constraint/symbolic_interaction_extension_again_2.txt'\n",
    "constraint_vb_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/constraint/verbalized_interaction_extension_again_2.txt'\n",
    "primitive_filter(filter_file_name, filter_verbalized_file_name, constraint_file_name, constraint_vb_file_name, concepts_accessibility)\n",
    "\n",
    "# Step 3-3: rule critic by GPT-4\n",
    "critic_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/critic/symbolic_interaction_extension_again_2.txt'\n",
    "critic_vb_file_name = 'ScriptData/Primitive/RuleSet/rule_base/complex_extension/critic/verbalized_interaction_extension_again_2.txt'\n",
    "rule_critic(constraint_file_name, constraint_vb_file_name, critic_file_name, critic_vb_file_name, rule_type=\"affordance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_extension(extension_file, use_self=None, use_self2=None):\n",
    "    with open(extension_file, \"r\") as r_f:\n",
    "        extension_rules = r_f.readlines()\n",
    "    if use_self:\n",
    "        with open(use_self, \"r\") as r_f_2:\n",
    "            extension_rules += r_f_2.readlines()\n",
    "    if use_self2:\n",
    "        with open(use_self2, \"r\") as r_f_3:\n",
    "            extension_rules += r_f_3.readlines()\n",
    "    \n",
    "    extension_dict = {}\n",
    "    for each_rule in extension_rules:\n",
    "        conclusion, premises_list = parse_sentence_to_conclusion_premise(each_rule)\n",
    "        assert len(premises_list) > 1\n",
    "\n",
    "        if conclusion not in extension_dict:\n",
    "            extension_dict[conclusion] = [premises_list]\n",
    "        else:\n",
    "            if premises_list not in extension_dict[conclusion]:\n",
    "                extension_dict[conclusion].append(premises_list)\n",
    "    print(len(extension_dict))\n",
    "    return extension_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def substitute_all_components(run_file_name, extension_file, write_file_name, variable_list=[\"A\", 'B'], use_self=None, use_self2=None):\n",
    "    extension_dict = substitute_extension(extension_file, use_self, use_self2)\n",
    "    with open(run_file_name, \"r\") as r_f:\n",
    "        rules = r_f.readlines()\n",
    "\n",
    "    extend_rules = []\n",
    "    extend_rules_len_stas = []\n",
    "    for each_rule in tqdm(rules):\n",
    "        # print(each_rule)\n",
    "        conclusion, premises_list = parse_sentence_to_conclusion_premise(each_rule)\n",
    "\n",
    "        for premise_index in range(len(premises_list)):  \n",
    "            each_rela, each_args_types, each_args_variables = argument_parsing(premises_list[premise_index], output_rela=True)\n",
    "        \n",
    "            each_key = each_rela + \"(\" + each_args_types[0] + \" X, \" + each_args_types[1] + \" Y)\"\n",
    "            each_key = each_key[0].upper() + each_key[1:]\n",
    "            if each_key in extension_dict:\n",
    "                for prem_extend_index in range(len(extension_dict[each_key])):\n",
    "                    each_prem_list = extension_dict[each_key][prem_extend_index]\n",
    "\n",
    "                    variable_start = 0\n",
    "                    map_variabels_dict = {}\n",
    "                    map_variabels_dict[\"X\"] = each_args_variables[0]\n",
    "                    map_variabels_dict[\"Y\"] = each_args_variables[1]\n",
    "\n",
    "                    rp_premise_list = []\n",
    "                    for each_sub_prem in each_prem_list:\n",
    "                        cur_rela, args_type_list, args_vairable_list = argument_parsing(each_sub_prem, output_rela=True)\n",
    "                        for j in range(len(args_vairable_list)):\n",
    "                            if args_vairable_list[j].upper() not in map_variabels_dict:\n",
    "                                map_variabels_dict[args_vairable_list[j].upper()] = variable_list[len(map_variabels_dict)-2+variable_start]\n",
    "                        if len(args_vairable_list) >= 2:\n",
    "                            each_prem_edit_variable = f\"{cur_rela}({args_type_list[0]} {map_variabels_dict[args_vairable_list[0]]}, {args_type_list[1]} {map_variabels_dict[args_vairable_list[1]]})\"\n",
    "                        else:\n",
    "                            each_prem_edit_variable = f\"{cur_rela}({args_type_list[0]} {map_variabels_dict[args_vairable_list[0]]})\"\n",
    "                        rp_premise_list.append(each_prem_edit_variable)\n",
    "                                    \n",
    "                    new_premises_list = premises_list[:premise_index] + rp_premise_list + premises_list[premise_index+1:]\n",
    "                    new_rule = conclusion + \":- \" + \", \".join(new_premises_list) + \";\"\n",
    "                    if new_rule not in extend_rules:\n",
    "                        extend_rules.append(new_rule)\n",
    "                        extend_rules_len_stas.append(len(new_premises_list))\n",
    "    print(len(extend_rules), sum(extend_rules_len_stas)/len(extend_rules_len_stas), max(extend_rules_len_stas), min(extend_rules_len_stas))\n",
    "\n",
    "    with open(write_file_name, 'w') as w_f:\n",
    "        for each in extend_rules:\n",
    "            w_f.write(each+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def substitute_two_premise(run_file_name, extension_file, write_file_name, variable_list = [\"A\", 'B', 'C', 'D', 'E', 'F', 'G', \"H\"], use_self=False):\n",
    "    extension_dict = substitute_extension(extension_file, use_self)\n",
    "    with open(run_file_name, \"r\") as r_f:\n",
    "        rules = r_f.readlines()\n",
    "\n",
    "    extend_rules = []\n",
    "    extend_rules_len_stas = []\n",
    "    for each_rule in tqdm(rules):\n",
    "        conclusion, premises_list = parse_sentence_to_conclusion_premise(each_rule)\n",
    "\n",
    "        replace_num = 0\n",
    "        variable_start = 0\n",
    "        new_premises_list = [[],]\n",
    "        premise_start = 0\n",
    "        for premise_index in range(len(premises_list)):  \n",
    "            each_rela, each_args_types, each_args_variables = argument_parsing(premises_list[premise_index], output_rela=True)\n",
    "        \n",
    "            each_key = each_rela + \"(\" + each_args_types[0] + \" X, \" + each_args_types[1] + \" Y)\"\n",
    "            each_key = each_key[0].upper() + each_key[1:]\n",
    "            if each_key in extension_dict:\n",
    "                replace_num += 1\n",
    "                temp_premise_lists = []\n",
    "                for prem_extend_index in range(min(len(extension_dict[each_key]), 3)):\n",
    "                    each_prem_list = extension_dict[each_key][prem_extend_index]\n",
    "\n",
    "                    map_variabels_dict = {}\n",
    "                    map_variabels_dict[\"X\"] = each_args_variables[0]\n",
    "                    map_variabels_dict[\"Y\"] = each_args_variables[1]\n",
    "                    rp_premise_list = []\n",
    "                    for each_sub_prem in each_prem_list:\n",
    "                        cur_rela, args_type_list, args_vairable_list = argument_parsing(each_sub_prem, output_rela=True)\n",
    "                        for j in range(len(args_vairable_list)):\n",
    "                            if args_vairable_list[j].upper() not in map_variabels_dict:\n",
    "                                map_variabels_dict[args_vairable_list[j].upper()] = variable_list[len(map_variabels_dict)-2+variable_start]\n",
    "                        each_prem_edit_variable = f\"{cur_rela}({args_type_list[0]} {map_variabels_dict[args_vairable_list[0]]}, {args_type_list[1]} {map_variabels_dict[args_vairable_list[1]]})\"\n",
    "                        rp_premise_list.append(each_prem_edit_variable)\n",
    "\n",
    "                    for n in range(len(new_premises_list)):\n",
    "                        temp_premise_lists.append(new_premises_list[n] + premises_list[premise_start:premise_index] + rp_premise_list)\n",
    "                    if len(map_variabels_dict) > 2:\n",
    "                        variable_start = variable_start + len(map_variabels_dict)-2\n",
    "                premise_start = premise_index + 1\n",
    "                new_premises_list = temp_premise_lists\n",
    "            else:\n",
    "                temp_premise_lists = []\n",
    "                for n in range(len(new_premises_list)):\n",
    "                    temp_premise_lists.append(new_premises_list[n] + premises_list[premise_index: premise_index+1])\n",
    "                new_premises_list = temp_premise_lists\n",
    "        for each_new_premises_list in new_premises_list:\n",
    "            new_rule = conclusion + \":- \" + \", \".join(each_new_premises_list) + \";\"\n",
    "            if new_rule not in extend_rules and replace_num > 1:\n",
    "                extend_rules.append(new_rule)\n",
    "                extend_rules_len_stas.append(len(each_new_premises_list))\n",
    "    print(len(extend_rules), sum(extend_rules_len_stas)/len(extend_rules_len_stas), max(extend_rules_len_stas), min(extend_rules_len_stas))\n",
    "\n",
    "    with open(write_file_name, 'w') as w_f:\n",
    "        for each in extend_rules:\n",
    "            w_f.write(each+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_file = \"ScriptData/Primitive/RuleSet/rule_base/final_rules/interaction_symbolic_pos_rules_allpossibility_strict.txt\"\n",
    "extension_file = \"ScriptData/Primitive/RuleSet/rule_base/final_rules/symbolic_interaction_extension_strict.txt\"\n",
    "one_step_bw_rule_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/interaction_symbolic_rules_one_bw_strict.txt\"\n",
    "substitute_all_components(rule_file, extension_file, one_step_bw_rule_file, use_self=False)\n",
    "\n",
    "rule_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/repetitive_filter/interaction_symbolic_rules_one_bw_strict.txt\"\n",
    "extension_file = \"ScriptData/Primitive/RuleSet/rule_base/final_rules/symbolic_interaction_extension_again_strict.txt\"\n",
    "two_step_bw_rule_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/interaction_symbolic_rules_two_bw_2_strict_useself.txt\"\n",
    "use_self = \"ScriptData/Primitive/RuleSet/rule_base/final_rules/symbolic_interaction_extension_strict.txt\"\n",
    "substitute_all_components(rule_file, extension_file, two_step_bw_rule_file, variable_list=[\"C\", 'D'], use_self=use_self)\n",
    "\n",
    "rule_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/repetitive_filter/interaction_symbolic_rules_two_bw_2_strict_useself.txt\"\n",
    "extension_file = \"ScriptData/Primitive/RuleSet/rule_base/final_rules/symbolic_interaction_extension_again_2_strict.txt\"\n",
    "three_step_bw_rule_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/interaction_symbolic_rules_three_bw_3_strict_useself.txt\"\n",
    "use_self = \"ScriptData/Primitive/RuleSet/rule_base/final_rules/symbolic_interaction_extension_strict.txt\"\n",
    "use_self2 = \"ScriptData/Primitive/RuleSet/rule_base/final_rules/symbolic_interaction_extension_again_strict.txt\"\n",
    "substitute_all_components(rule_file, extension_file, three_step_bw_rule_file, variable_list=[\"E\", 'F'], use_self=use_self, use_self2=use_self2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter repetitive extended rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_repetitive_rules(extend_rule_file, filter_extend_rule_file, thres=3):\n",
    "    with open(extend_rule_file, \"r\") as r_f:\n",
    "        diversified_rules = r_f.readlines()\n",
    "    print(len(diversified_rules))\n",
    "\n",
    "    filtered_rules = []\n",
    "    for each_rule in tqdm(diversified_rules):\n",
    "        each_rule = each_rule.strip()\n",
    "        conclusion, premises_list = parse_sentence_to_conclusion_premise(each_rule)\n",
    "\n",
    "        conc_rela, _, _ = argument_parsing(conclusion, output_rela=True)\n",
    "        premise_rela_list = []\n",
    "        if len(set(premises_list)) < len(premises_list):\n",
    "            continue\n",
    "        for i in range(len(premises_list)):\n",
    "            each_rela, each_args_types, each_args_variables = argument_parsing(premises_list[i], output_rela=True)\n",
    "            assert len(each_args_variables) == 2 and len(each_args_types) == 2\n",
    "            premise_rela_list.append(each_rela.lower())\n",
    "        # if len(set(premise_rela_list)) < len(premise_rela_list):\n",
    "        #     continue\n",
    "        _, max_similarity = get_most_similar_premise(conc_rela.lower(), premise_rela_list)\n",
    "        if max_similarity <= thres:\n",
    "            filtered_rules.append(each_rule)\n",
    "    print(len(filtered_rules))\n",
    "\n",
    "    with open(filter_extend_rule_file, \"w\") as w_f:\n",
    "        for each in tqdm(filtered_rules):\n",
    "            w_f.write(each+\"\\n\")\n",
    "\n",
    "def remove_character(text):\n",
    "    for character in [\"X\", \"Y\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]:\n",
    "        text = text.replace(\" \" + character, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def filter_same_rules(extend_rule_file, filter_extend_rule_file, thres=4):\n",
    "    with open(extend_rule_file, \"r\") as r_f:\n",
    "        diversified_rules = r_f.readlines()\n",
    "    print(len(diversified_rules))\n",
    "\n",
    "    unique_rules = []\n",
    "    filtered_rules = []\n",
    "    for each_rule in tqdm(diversified_rules):\n",
    "        each_rule = each_rule.strip()\n",
    "\n",
    "        cur_unique_rule = remove_character(each_rule)\n",
    "        if cur_unique_rule not in unique_rules:\n",
    "            unique_rules.append(cur_unique_rule)\n",
    "        \n",
    "            conclusion, premises_list = parse_sentence_to_conclusion_premise(each_rule)\n",
    "\n",
    "            conc_rela, _, _ = argument_parsing(conclusion, output_rela=True)\n",
    "            premise_rela_list = []\n",
    "            if len(set(premises_list)) < len(premises_list):\n",
    "                continue\n",
    "            for i in range(len(premises_list)):\n",
    "                each_rela, each_args_types, each_args_variables = argument_parsing(premises_list[i], output_rela=True)\n",
    "                premise_rela_list.append(each_rela.lower())\n",
    "            if \"same\" in premise_rela_list or \"sameas\" in premise_rela_list or \"similar\" in premise_rela_list or \"similarto\" in premise_rela_list or \"equivalent\" in premise_rela_list:\n",
    "                continue\n",
    "            else:\n",
    "                _, max_similarity = get_most_similar_premise(conc_rela.lower(), premise_rela_list)\n",
    "                if max_similarity <= thres and each_rule not in filtered_rules:\n",
    "                    filtered_rules.append(each_rule)\n",
    "    print(len(filtered_rules))\n",
    "\n",
    "    with open(filter_extend_rule_file, \"w\") as w_f:\n",
    "        for each in tqdm(filtered_rules):\n",
    "            w_f.write(each+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_whole_verbalize_input(each_premise):\n",
    "    input = \"Please verbalize each input rule into a natural langauge statement in 'if-then' format. \\n\\nExamples:\\n\" + \\\n",
    "            \"Rule:\\n\" + \\\n",
    "            \"CanRequest(Person X, Authorization Y):- Have(Person X, Age Z1), RequireMinimumAge(Authorization Y, Age Z2), BiggerThan(Age Z1, Age Z2);\\n\" + \\\n",
    "            \"Statement:\\n\" + \\\n",
    "            \"If Person X has Age Z1 and the minimum age requirement for requesting Authorization Y is Age Z2, Age Z1 is bigger than Age Z2, then Person X can request Authorization Y. \\n\" + \\\n",
    "            \"Rule:\\n\" + \\\n",
    "            \"CanRepair(Person X, Electronic Device Y):- Master(Person X, Skill Z2), RequiredForRepairing(Skill Z2, Electronic Device Y);\\n\" + \\\n",
    "            \"Statement:\\n\" + \\\n",
    "            \"If Person X has mastered Skill Z2 and Skill Z2 is required for repairing Electronic Device Y, then Person X can repair Electronic Device Y.\\n\\n\" + \\\n",
    "            \"Rule:\\n\" + \\\n",
    "            each_premise + \"\\n\" + \\\n",
    "            \"Statement:\\n\"\n",
    "    return input\n",
    "\n",
    "def verbalized_extended_rules(file_name, write_verbalized_rule_file):\n",
    "    with open(file_name, \"r\") as r_f:\n",
    "        rules = r_f.readlines()\n",
    "\n",
    "    with open(write_verbalized_rule_file, \"w\") as w_f:\n",
    "        for each_rule in tqdm(rules):\n",
    "            input = get_whole_verbalize_input(each_rule.strip())\n",
    "            response = get_GPT4_response(input, max_tokens=100, temp=0)\n",
    "            w_f.write(response+\"\\n\")\n",
    "\n",
    "def get_prem_verbalize_input(each_premise):\n",
    "    input = \"Please verbalize input facts into a natural langauge statement. \\n\\nExamples:\\n\" + \\\n",
    "            \"Facts:\\n\" + \\\n",
    "            \"Have(Person X, Age Z1), RequireMinimumAge(Authorization Y, Age Z2), BiggerThan(Age Z1, Age Z2);\\n\" + \\\n",
    "            \"Statement:\\n\" + \\\n",
    "            \"Person X has Age Z1 and the minimum age requirement for requesting Authorization Y is Age Z2, Age Z1 is bigger than Age Z2. \\n\" + \\\n",
    "            \"Facts:\\n\" + \\\n",
    "            \"Master(Person X, Skill Z2), RequiredForRepairing(Skill Z2, Electronic Device Y);\\n\" + \\\n",
    "            \"Statement:\\n\" + \\\n",
    "            \"Person X has mastered Skill Z2, and Skill Z2 is required for repairing Electronic Device Y.\\n\\n\" + \\\n",
    "            \"Facts:\\n\" + \\\n",
    "            each_premise + \"\\n\" + \\\n",
    "            \"Statement:\\n\"\n",
    "    return input\n",
    "\n",
    "def get_conc_verbalize_input(each_premise):\n",
    "    input = \"Please verbalize each input fact into a natural langauge statement. \\n\\nExamples:\\n\" + \\\n",
    "            \"Fact: IsProficientIn(Person X, Skill Y)\\n\" + \\\n",
    "            \"Statement:\\n\" + \\\n",
    "            \"Person X has a level of proficiency or expertise in Skill Y. \\n\" + \\\n",
    "            \"Fact: ReleasedIn(Item X, Year A)\\n\" + \\\n",
    "            \"Statement:\\n\" + \\\n",
    "            \"Item X was released in the year A.\\n\" + \\\n",
    "            \"Fact: CanUse(Person B, Item Y)\\n\" + \\\n",
    "            \"Statement:\\n\" + \\\n",
    "            \"Person B can use Item Y. \\n\" + \\\n",
    "            \"Fact: \" + each_premise + \"\\n\" + \\\n",
    "            \"Statement:\\n\"\n",
    "    return input\n",
    "\n",
    "def separate_verbalized_extended_rules(file_name, write_verbalized_rule_file):\n",
    "    with open(file_name, \"r\") as r_f:\n",
    "        rules = r_f.readlines()\n",
    "\n",
    "    with open(write_verbalized_rule_file, \"w\") as w_f:\n",
    "        for each_rule in tqdm(rules):\n",
    "            conclusion, _ = each_rule.split(\":- \")\n",
    "            conclusion = conclusion.strip()\n",
    "            all_text = get_GPT4_response(get_whole_verbalize_input(each_rule.strip()), max_tokens=100, temp=0.1)\n",
    "            assert all_text[:3] == \"If \" and \", then \" in all_text\n",
    "            premise_text = all_text.split(\", then \")[0] \n",
    "            conc_text = get_GPT4_response(get_conc_verbalize_input(conclusion), max_tokens=100, temp=0.1)\n",
    "            rule_text = premise_text + \", then \" + conc_text\n",
    "            # print(each_rule)\n",
    "            # print(rule_text)\n",
    "            # print(\"*\"*100)\n",
    "            w_f.write(rule_text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_concept(text, concepts):\n",
    "    for each in list(concepts.keys()) + [\"Person\", \"Region\"]:\n",
    "        if each+\" \" in text:\n",
    "            text = text.replace(each+\" \", \"\")\n",
    "    text = text.replace(\"X \", \"\").replace(\"X\", \"\").replace(\"Y \", \"\").replace(\"Y\", \"\")\n",
    "    return text\n",
    "\n",
    "def remove_character(text):\n",
    "    for character in [\"X\", \"Y\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]:\n",
    "        text = text.replace(\" \" + character, \"\")\n",
    "    return text\n",
    "\n",
    "def filter_diversified_verblized_rules(rule_file, vb_rule_file, filter_file_name, filter_vb_file_name, concepts):\n",
    "    with open(rule_file, \"r\") as r_f:\n",
    "        rules = r_f.readlines()\n",
    "    with open(vb_rule_file, \"r\") as r_f:\n",
    "        vb_rules = r_f.readlines()\n",
    "    print(len(rules), len(vb_rules))\n",
    "\n",
    "    unique_rules = []\n",
    "    filtered_rules = []\n",
    "    filtered_vb_rules = []\n",
    "    for n, each_rule in tqdm(enumerate(vb_rules)):\n",
    "        each_rule = each_rule.strip()\n",
    "        assert each_rule[:3] == \"If \" and \", then \" in each_rule\n",
    "        premise, conclusion = each_rule[3:].split(\", then \")\n",
    "        premise = remove_concept(premise, concepts).lower()\n",
    "        conclusion = remove_concept(conclusion, concepts).lower()\n",
    "\n",
    "        cur_unique_rule = remove_character(rules[n])\n",
    "        if cur_unique_rule not in unique_rules:\n",
    "            unique_rules.append(cur_unique_rule)\n",
    "            similarity = find_lcseque(premise, conclusion)\n",
    "            if similarity <= 15: # multiple 15 single 12\n",
    "                filtered_rules.append(rules[n].strip())\n",
    "                filtered_vb_rules.append(each_rule)\n",
    "    print(len(filtered_rules), len(filtered_vb_rules))\n",
    "\n",
    "    with open(filter_file_name, \"w\") as w_f:\n",
    "        for each in tqdm(filtered_rules):\n",
    "            w_f.write(each+\"\\n\")\n",
    "    with open(filter_vb_file_name, \"w\") as w_f:\n",
    "        for each in tqdm(filtered_vb_rules):\n",
    "            w_f.write(each+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepara_annotation_data(verbalized_rule_file, annotation_file):\n",
    "    with open(verbalized_rule_file, \"r\") as v_r_f:\n",
    "        verbalized_rules = v_r_f.readlines()\n",
    "        print(len(verbalized_rules))\n",
    "\n",
    "    selected_index = range(len(verbalized_rules))\n",
    "    # selected_index = range(200)\n",
    "\n",
    "    data_list = []\n",
    "    for i in selected_index:\n",
    "        each_data = {}\n",
    "        each_data['id'] = i\n",
    "        each_rule = verbalized_rules[i].strip()\n",
    "        assert each_rule[:3] == \"If \" and \", then \" in each_rule\n",
    "        premise, conclusion = each_rule[3:].split(\", then \")\n",
    "        premise = premise + \".\"\n",
    "        each_data['premise'] = premise\n",
    "        each_data['conclusion'] = conclusion\n",
    "        data_list.append(each_data)\n",
    "    print(len(data_list))    \n",
    "\n",
    "    import csv\n",
    "    with open(annotation_file, \"w\") as csv_f:\n",
    "        writer = csv.writer(csv_f)\n",
    "        writer.writerow(['entryid', 'premise', 'conclusion'])\n",
    "\n",
    "        for each in data_list:\n",
    "            writer.writerow([each['id'], each['premise'], each['conclusion']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "def process_annotation_data(rule_file, verbalized_rule_file, result_file, write_file_name, write_verbalized_file_name, strict=False, post_filter=False):\n",
    "    with open(rule_file, \"r\") as v_r_f:\n",
    "        rules = v_r_f.readlines()\n",
    "        print(len(rules))\n",
    "    with open(verbalized_rule_file, \"r\") as v_r_f:\n",
    "        verbalized_rules = v_r_f.readlines()\n",
    "        print(len(verbalized_rules))\n",
    "\n",
    "    all_data = []\n",
    "    with open(result_file) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            all_data.append(row)\n",
    "    print(len(all_data)-1)\n",
    "    print(\"worker_id\", all_data[0].index(\"WorkerId\"))\n",
    "\n",
    "    annotation_dict_correct = {}\n",
    "    annotation_dict_common_premise = {}\n",
    "    annotation_dict_common_conc = {}\n",
    "    for i in range(1, len(all_data)):\n",
    "        if all_data[i][15] != 'A3MTHVR1EJ8LMM':\n",
    "            entry_id = all_data[i][27]\n",
    "            if entry_id not in annotation_dict_correct:\n",
    "                annotation_dict_correct[entry_id] = [all_data[i][32]]\n",
    "            else:\n",
    "                annotation_dict_correct[entry_id].append(all_data[i][32]) \n",
    "            \n",
    "            if entry_id not in annotation_dict_common_premise:\n",
    "                annotation_dict_common_premise[entry_id] = [all_data[i][33]]\n",
    "            else:\n",
    "                annotation_dict_common_premise[entry_id].append(all_data[i][33]) \n",
    "            \n",
    "            if entry_id not in annotation_dict_common_conc:\n",
    "                annotation_dict_common_conc[entry_id] = [all_data[i][34]]\n",
    "            else:\n",
    "                annotation_dict_common_conc[entry_id].append(all_data[i][34]) \n",
    "    print(len(annotation_dict_correct), len(annotation_dict_common_premise), len(annotation_dict_common_conc))\n",
    "\n",
    "    all_annotate_ids = []\n",
    "    filter_ids_1 = []\n",
    "    for each in annotation_dict_correct:\n",
    "        if len(annotation_dict_correct[each]) < 3:\n",
    "            all_annotate_ids.append(each)\n",
    "\n",
    "        entaiment_count = annotation_dict_correct[each].count(\"2\")\n",
    "        premise_count = annotation_dict_common_premise[each].count(\"2\")\n",
    "        conc_count = annotation_dict_common_conc[each].count(\"2\")\n",
    "        if not strict:\n",
    "            if premise_count >= 2 and conc_count >= 2 and entaiment_count >= 2:   # multiple \n",
    "                filter_ids_1.append(each)\n",
    "        else:\n",
    "            if premise_count >= 2 and conc_count >= 2 and entaiment_count >= 3:   # multiple \n",
    "                filter_ids_1.append(each)\n",
    "    correct_index = sorted(filter_ids_1)\n",
    "\n",
    "    if post_filter:\n",
    "        new_correct_index = []\n",
    "        for each_id in correct_index:\n",
    "            conclusion, premises_list = parse_sentence_to_conclusion_premise(rules[int(each_id)].strip())\n",
    "            premise, vb_conclusion = verbalized_rules[int(each_id)].strip()[3:].split(\", then \") \n",
    "            if \"Can\" not in conclusion and \" can \" in vb_conclusion:\n",
    "                pass\n",
    "            else:\n",
    "                new_correct_index.append(each_id)\n",
    "        correct_index = new_correct_index\n",
    "\n",
    "    print(len(correct_index))\n",
    "    print(\"all_annotate_ids\", all_annotate_ids)\n",
    "\n",
    "    with open(write_file_name, 'w') as w_f:\n",
    "        for id in correct_index:\n",
    "            w_f.write(rules[int(id)].strip()+\"\\n\")\n",
    "    with open(write_verbalized_file_name, 'w') as w_f:\n",
    "        for id in correct_index:\n",
    "            w_f.write(verbalized_rules[int(id)].strip()+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Step \n",
    "one_step_bw_rule_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/interaction_symbolic_rules_one_bw_strict.txt\"\n",
    "# Step 4-4: filter repetitive extended rules\n",
    "filter_symbolic_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/filter/interaction_symbolic_rules_one_bw_strict.txt'\n",
    "filter_same_rules(one_step_bw_rule_file, filter_symbolic_extend_rule_file, thres=4)\n",
    "\n",
    "# Step 4-5: verbalize extended rules\n",
    "filter_verbalized_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/filter/interaction_verbalized_rules_one_bw_strict.txt'\n",
    "verbalized_extended_rules(filter_symbolic_extend_rule_file, filter_verbalized_extend_rule_file)\n",
    "\n",
    "# Step 4-6: filter repetitive verbalized extended rules\n",
    "re_filter_symbolic_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/repetitive_filter/interaction_symbolic_rules_one_bw_strict.txt'\n",
    "re_filter_verbalized_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/repetitive_filter/interaction_verbalized_rules_one_bw_strict.txt'\n",
    "filter_diversified_verblized_rules(filter_symbolic_extend_rule_file, filter_verbalized_extend_rule_file, re_filter_symbolic_extend_rule_file, re_filter_verbalized_extend_rule_file, concepts_accessibility)\n",
    "\n",
    "filter_invalid_rule(re_filter_symbolic_extend_rule_file, re_filter_symbolic_extend_rule_file, re_filter_verbalized_extend_rule_file, re_filter_verbalized_extend_rule_file, is_single=False, is_compositional=True)\n",
    "\n",
    "\n",
    "# Two Step \n",
    "two_step_bw_rule_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/interaction_symbolic_rules_two_bw_2_strict_useself.txt\"\n",
    "# Step 4-4: filter repetitive extended rules\n",
    "filter_symbolic_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/filter/interaction_symbolic_rules_two_bw_2_strict_useself.txt'\n",
    "filter_same_rules(two_step_bw_rule_file, filter_symbolic_extend_rule_file, thres=10)\n",
    "\n",
    "# Step 4-5: verbalize extended rules\n",
    "filter_verbalized_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/filter/interaction_verbalized_rules_two_bw_2_strict_useself.txt'\n",
    "verbalized_extended_rules(filter_symbolic_extend_rule_file, filter_verbalized_extend_rule_file)\n",
    "\n",
    "# Step 4-6: filter repetitive verbalized extended rules\n",
    "re_filter_symbolic_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/repetitive_filter/interaction_symbolic_rules_two_bw_2_strict_useself.txt'\n",
    "re_filter_verbalized_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/repetitive_filter/interaction_verbalized_rules_two_bw_2_strict_useself.txt'\n",
    "filter_diversified_verblized_rules(filter_symbolic_extend_rule_file, filter_verbalized_extend_rule_file, re_filter_symbolic_extend_rule_file, re_filter_verbalized_extend_rule_file, concepts_accessibility)\n",
    "\n",
    "filter_invalid_rule(re_filter_symbolic_extend_rule_file, re_filter_symbolic_extend_rule_file, re_filter_verbalized_extend_rule_file, re_filter_verbalized_extend_rule_file, is_single=False, is_compositional=True)\n",
    "\n",
    "\n",
    "# Three Step v3\n",
    "three_step_bw_rule_file = \"ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/interaction_symbolic_rules_three_bw_3_strict_useself.txt\"\n",
    "# Step 4-4: filter repetitive extended rules\n",
    "filter_symbolic_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/filter/interaction_symbolic_rules_three_bw_3_strict_useself.txt'\n",
    "filter_same_rules(three_step_bw_rule_file, filter_symbolic_extend_rule_file, thres=10)\n",
    "\n",
    "# Step 4-5: verbalize extended rules\n",
    "filter_verbalized_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/filter/interaction_verbalized_rules_three_bw_3_strict_useself.txt'\n",
    "verbalized_extended_rules(filter_symbolic_extend_rule_file, filter_verbalized_extend_rule_file)\n",
    "\n",
    "# Step 4-6: filter repetitive verbalized extended rules\n",
    "re_filter_symbolic_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/repetitive_filter/interaction_symbolic_rules_three_bw_3_strict_useself.txt'\n",
    "re_filter_verbalized_extend_rule_file = 'ScriptData/Primitive/RuleSet/rule_base/complex_extended_rules/bw/repetitive_filter/interaction_verbalized_rules_three_bw_3_strict_useself.txt'\n",
    "filter_diversified_verblized_rules(filter_symbolic_extend_rule_file, filter_verbalized_extend_rule_file, re_filter_symbolic_extend_rule_file, re_filter_verbalized_extend_rule_file, concepts_location)\n",
    "\n",
    "filter_invalid_rule(re_filter_symbolic_extend_rule_file, re_filter_symbolic_extend_rule_file, re_filter_verbalized_extend_rule_file, re_filter_verbalized_extend_rule_file, is_single=False, is_compositional=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78e48a7539a3cb4c61f94c606d308a5205252ec1cba00f52b696918ed6d3e76e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
